{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weijiesun/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/weijiesun/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# import necessary pacakages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lib.neural_ode_surv import *\n",
    "from lib.utils import *\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check for available GPUs\n",
    "# DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to multiple events prediction example : Framingham data\n",
    "### Loading and pre-processing the data\n",
    "Event of interest : ANYCHD (Angina Pectoris, Myocardial infarction, Coronary Insufficiency, or Fatal Coronary Heart Disease)\n",
    "\n",
    "Competing event : Death from any cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3457521/3845909018.py:68: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '34.405' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data_train.loc[data_train[feat] < min_feat, feat] = min_feat\n",
      "/tmp/ipykernel_3457521/3845909018.py:72: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '34.405' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data_valid.loc[data_valid[feat] < min_feat, feat] = min_feat\n",
      "/tmp/ipykernel_3457521/3845909018.py:74: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '34.405' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data_test.loc[data_test[feat] < min_feat, feat] = min_feat\n"
     ]
    }
   ],
   "source": [
    "# For more details on the dataset and covariates in Framingham data, \n",
    "# see : https://biolincc.nhlbi.nih.gov/media/teachingstudies/FHS_Teaching_Longitudinal_Data_Documentation_2021a.pdf?link_time=2022-02-03_18:20:47.023970\n",
    "# this publicly available data has a person-time format for longitudinal measurements \n",
    "df_framingham = pd.read_csv('data/framingham.csv') \n",
    "\n",
    "# Specify categorical features\n",
    "feat_cat = ['SEX', 'CIGPDAY', 'CURSMOKE','educ', 'DIABETES', 'PREVSTRK', 'PREVHYP', 'BPMEDS']\n",
    "# Specify continuous features\n",
    "feat_cont = ['AGE', 'SYSBP', 'DIABP', 'TOTCHOL', 'HDLC', 'LDLC', 'BMI', 'GLUCOSE', 'HEARTRTE']\n",
    "# Specify features which SurvLatent ODE is set to reconstruct\n",
    "feat_reconstr = ['AGE', 'SYSBP', 'DIABP', 'TOTCHOL', 'HDLC', 'LDLC', 'BMI', 'GLUCOSE', 'HEARTRTE']#['SYSBP', 'DIABP', 'TOTCHOL', 'HDLC', 'LDLC', 'BMI', 'GLUCOSE', 'HEARTRTE']\n",
    "# Specify data_info_dic as follows \n",
    "# id_col : unique identifier for a patient\n",
    "# event_col : columns correspond to event indicator; this should be binary\n",
    "# time_col : column corresponds to time of measurement\n",
    "# time_to_event_col : column corresponds to observed time to event (i.e. t_i = min(T_i, C_i))\n",
    "# feat_cat : list containing a set of categorical features\n",
    "# feat_cont : list containing a set of continuous valued features\n",
    "data_info_dic = {'id_col':'RANDID', 'event_col':['ANYCHD', 'DEATH'], 'time_to_event_col':['TIMECHD', 'TIMEDTH'],\n",
    "                 'time_col':'TIME', 'feat_cat':feat_cat, 'feat_cont':feat_cont}\n",
    "\n",
    "feats_dim = len(feat_cat) + len(feat_cont)\n",
    "reconstr_dim = len(feat_reconstr)\n",
    "if type(data_info_dic['event_col']) == list:\n",
    "    n_events = len(data_info_dic['event_col'])\n",
    "else:\n",
    "    n_events = 1\n",
    "# Given that the Framingham study is a long follow-up study which spans about 20 years (or around 7500 days),  \n",
    "# we discretize follow-up time by 10 days. Therefore, our time unit is 10-day.\n",
    "df_framingham[data_info_dic['time_col']] = np.round(df_framingham[data_info_dic['time_col']].values/10)\n",
    "df_framingham[data_info_dic['time_to_event_col']] = np.round(df_framingham[data_info_dic['time_to_event_col']].values/10)\n",
    "\n",
    "# We perform 0.65-0.15-0.2 (train-valid-test) split\n",
    "test_set_frac = 0.2; train_set_frac = 0.65\n",
    "random_seed = 1991 # set random seed for reproducibility\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "sample_ids = set(df_framingham.RANDID.values)\n",
    "sample_ids_test = set(np.random.choice(list(sample_ids), size = int(len(sample_ids)*test_set_frac), replace = False))\n",
    "sample_ids_train = \tset(np.random.choice(list(sample_ids - sample_ids_test),\n",
    "                                         size = int(len(sample_ids - sample_ids_test)*train_set_frac/(1-test_set_frac)),\n",
    "                                         replace = False))\n",
    "sample_ids_valid = sample_ids - sample_ids_test - sample_ids_train\n",
    "\n",
    "data_test = (df_framingham.loc[df_framingham.RANDID.isin(sample_ids_test)].\n",
    "             sort_values([data_info_dic['id_col'], data_info_dic['time_col']], ascending=(True, True)))\n",
    "data_train = (df_framingham.loc[df_framingham.RANDID.isin(sample_ids_train)].\n",
    "              sort_values([data_info_dic['id_col'], data_info_dic['time_col']], ascending=(True, True)))\n",
    "data_valid = (df_framingham.loc[df_framingham.RANDID.isin(sample_ids_valid)].\n",
    "              sort_values([data_info_dic['id_col'], data_info_dic['time_col']], ascending=(True, True)))\n",
    "\n",
    "######\n",
    "data_train = data_train.drop_duplicates('RANDID')\n",
    "data_valid = data_valid.drop_duplicates('RANDID')\n",
    "data_train = data_train.fillna(data_train.mean())\n",
    "data_valid = data_valid.fillna(data_valid.mean())\n",
    "data_train = data_train.fillna(0)\n",
    "data_valid = data_valid.fillna(0)\n",
    "######\n",
    "# outlier processing\n",
    "# We threshold outliers (i.e. feature vals < 0.005 percentile of corresponding feature vals in training set \n",
    "# AND feature vals > 0.995 percentile of corresponding features vals in training set)\n",
    "feats_oi = feat_cont + ['CIGPDAY']\n",
    "feat_to_min_max_dict = {}\n",
    "for feat in feats_oi:\n",
    "    min_feat = np.quantile(data_train[feat].dropna().values, q=0.005)\n",
    "    max_feat = np.quantile(data_train[feat].dropna().values, q=0.995)\n",
    "    data_train.loc[data_train[feat] < min_feat, feat] = min_feat\n",
    "    data_train.loc[data_train[feat] > max_feat, feat] = max_feat\n",
    "    feat_to_min_max_dict[feat] = (min_feat, max_feat)\n",
    "    # control outliers in the valid + test cohorts using training set\n",
    "    data_valid.loc[data_valid[feat] < min_feat, feat] = min_feat\n",
    "    data_valid.loc[data_valid[feat] > max_feat, feat] = max_feat\n",
    "    data_test.loc[data_test[feat] < min_feat, feat] = min_feat\n",
    "    data_test.loc[data_test[feat] > max_feat, feat] = max_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model hyperparameters and instantiate the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr : learning rate\n",
    "# surv_loss_scale : determines the scaling factor for the survival loss in the total loss\n",
    "# wait_until_full_surv_loss : wait # epochs until the full survival loss scaling, which allows the model to learn input representation before tuning survival estimates.\n",
    "batch_size = 100; lr = 0.01; surv_loss_scale = 100; wait_until_full_surv_loss = 3; early_stopping = True;\n",
    "# ODE-RNN encoder\n",
    "# enc_f_nn_layers : # of layers in the neural networks function f() for learning the latent dynamics on the encoder side\n",
    "# enc_latent_dim : dimensionality in the latent embedding on the encoder side\n",
    "# num_units_gru : # of units in each GRU cell\n",
    "enc_latent_dim = 50; enc_f_nn_layers = 5; num_units_gru = 80; \n",
    "\n",
    "# Decoder \n",
    "# dec_g_nn_layers : # of layers in the neural networks function g() for learning the latent dynamics on the decoder side\n",
    "# dec_latent_dim : dimensionality in the latent embedding on the decoder side\n",
    "# haz_dec_layers : # of layers in the cause-specific decoder module for hazard estimation\n",
    "# num_units_ode : # of units in function f() and g()\n",
    "dec_g_nn_layers = 7; dec_latent_dim = 40; haz_dec_layers = 3; num_units_ode = 70\n",
    "\n",
    "# Specify the prediction window to 8000 days from the entry with 10-day as a unit time\n",
    "max_pred_window = 800\n",
    "n_epochs =30 # number of training epochs\n",
    "\n",
    "# reconstr_dim = 0\n",
    "# del data_info_dic['feat_cont']\n",
    "# data_info_dic['feat_cont'] = []\n",
    "# feat_reconstr = []\n",
    "# feats_dim = len(feat_cat)\n",
    "# instantiate the model :\n",
    "model = SurvLatentODE(input_dim=feats_dim, reconstr_dim=reconstr_dim, dec_latent_dim=dec_latent_dim,\n",
    "                      enc_latent_dim=enc_latent_dim, enc_f_nn_layers=enc_f_nn_layers, \n",
    "                      dec_g_nn_layers=dec_g_nn_layers, num_units_ode=num_units_ode, num_units_gru=num_units_gru,\n",
    "                      device=DEVICE, n_events=n_events, haz_dec_layers=haz_dec_layers)\n",
    "# set the unique identifier for the corresponding training\n",
    "run_id = 'framingham_competing_events_example_v1_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing data...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2882/2882 [00:00<00:00, 28952.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded samples due to event times overlapping last observation times (i.e. remaining t_i = 0) :  [76273, 208566, 231492, 428306, 445400, 556045, 571377, 599475, 797308, 946128, 972314, 1075504, 1080397, 1232204, 1263082, 1406606, 1568334, 1663651, 1695438, 1798396, 1954038, 2087324, 2108588, 2134396, 2180046, 2181152, 2408348, 2434794, 2448708, 2474378, 2483517, 2507740, 2564697, 2640601, 2646666, 2682411, 2708769, 2727755, 2839250, 2865166, 2951629, 3117784, 3235453, 3361368, 3455001, 3587516, 3603542, 3683894, 3702628, 3710385, 3779954, 3810088, 3915943, 3927641, 4030316, 4066905, 4210168, 4220542, 4227351, 4229307, 4244133, 4295921, 4362066, 4504564, 4543147, 4637896, 4645346, 4700914, 4719511, 4754059, 4757585, 4897828, 4903592, 5130897, 5187398, 5266590, 5386797, 5406199, 5561922, 5571109, 5610759, 5611619, 5654796, 5784616, 5837883, 5871074, 5917953, 6171121, 6442383, 6662624, 6720746, 6759187, 7165088, 7173129, 7215259, 7392212, 7414089, 7416933, 7447033, 7507638, 7568367, 7670056, 7671700, 7730009, 8009750, 8037533, 8276879, 8359476, 8369928, 8511124, 8620013, 8693562, 8703932, 8856516, 8894297, 9115104, 9184034, 9187843, 9211350, 9211709, 9309268, 9524061, 9550624, 9551094, 9562666, 9570274, 9580415, 9679810, 9746116, 9838321, 9964282]\n",
      "n =  131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing data...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 666/666 [00:00<00:00, 27577.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded samples due to event times overlapping last observation times (i.e. remaining t_i = 0) :  [414678, 512339, 1101060, 2080190, 2097493, 3158323, 3718050, 4233073, 4588247, 4614345, 4726021, 5080716, 5865112, 6316687, 6949688, 7410564, 7546881, 7559394, 7668298, 8083473, 8466833, 9380233, 9802787, 9868819]\n",
      "n =  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weijiesun/survival_project/survlatent_ode/lib/utils.py\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using currently existing directory :  model_performance/framingham_competing_events_example_v1_1\n",
      "using currently existing directory :  surv_curves/framingham_competing_events_example_v1_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training across 30 epochs:   3%|████▍                                                                                                                                    | 27/840 [03:39<49:30,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading validation set...:   0%|                                                                                                                                                    | 0/4 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "# note that samples with event times of zero (i.e. t_i = 0; event time overlapping the latest observation times) are excluded.\n",
    "model.fit(data_train, data_valid, data_info_dic,\n",
    "          max_pred_window=max_pred_window, run_id=run_id, n_epochs=n_epochs,\n",
    "          batch_size=batch_size, surv_loss_scale=surv_loss_scale, early_stopping=early_stopping,\n",
    "          feat_reconstr=feat_reconstr, wait_until_full_surv_loss=wait_until_full_surv_loss, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.drop_duplicates('RANDID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "print('Loading the trained model...')\n",
    "print('run_id : ', run_id)\n",
    "path = 'model_performance/' + run_id + '/best_model.pt'\n",
    "try:\n",
    "    model_info = get_ckpt_model(path, model, DEVICE)\n",
    "except:\n",
    "    raise KeyError('Model not found...')\n",
    "\n",
    "# Process the held-out test set \n",
    "batch_dict_test = model.process_eval_data(data_test, data_info_dic, max_pred_window=max_pred_window,\n",
    "                                          run_id=run_id, feat_reconstr=feat_reconstr, model_info=model_info)\n",
    "# Get estimated survival probabilities\n",
    "# Note : survival probs are estimated from the latest observation for each sample\n",
    "# due to generative nature, surv probs may be different across runs. In this example, we set the random seed to control non-deterministic elements\n",
    "ef_surv_prob, cs_cif_total = model.get_surv_prob(batch_dict_test, model_info=model_info,\n",
    "                                                 max_pred_window=max_pred_window, filename_suffix=run_id,\n",
    "                                                 device=DEVICE, n_events=n_events)\n",
    "# Evaluate the model on the held-out set and obtain model performance summary\n",
    "df_test_result_comp = eval_model(model_info, batch_dict_test, ef_surv_prob, run_id=run_id,\n",
    "                                 cs_cif_total=cs_cif_total, max_pred_window=max_pred_window, n_events=n_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_result_comp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
